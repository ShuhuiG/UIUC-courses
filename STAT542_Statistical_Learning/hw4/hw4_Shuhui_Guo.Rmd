---
title: "STAT 542, homework 4"
author: "Shuhui Guo"
fontsize: 12pt
output: pdf_document
---

# Question 1

```{r, echo = F, include = F}
library(ggplot2)
library(readr)
```

##a)

Consider the Gaussian kernel function:

\begin{align*}
K(u)=1/\sqrt{2\pi}exp(-u^{2}/2)
\end{align*}

Let $u=|x-x_{i}|/\lambda$ , the Nadaraya-Watson kernel regression estimator can be written as:

\begin{align*}
\hat{f(x)}=\frac{\sum_{i}K_{\lambda}(x,x_{i})y_{i}}{\sum_{i}K_{\lambda}(x,x_{i})}=\frac{\sum_{i}K(|x-x_{i}|/\lambda)y_{i}}{\sum_{i}K(|x-x_{i}|/\lambda)}=\frac{\sum_{i}exp[-(x-x_{i})^2/(2\lambda^2)]y_{i}}{\sum_{i}exp[-(x-x_{i})^2/(2\lambda^2)]}
\end{align*}

I wrote my own code to fit the Nadaraya-Watson kernel regression estimator using Gaussian kernel for one dimensional problem. To apply my code, I generated a toy data example which contains the training data and testing data. In the training data, there are 1000 points of $x$ randomly in $[0,2\pi]$ and 1000 points of $y$ generated by

\begin{align*}
y=2sin(x)+\epsilon, \quad \epsilon \sim N(0,1)
\end{align*}

For the testing data, I regenerated a new dataset of $x$ and $y$ using the above methods.

```{r, echo = F}
set.seed(1)
# generate some data
x <- runif(1000, 0, 2*pi)
y <- 2*sin(x) + rnorm(length(x))

# generate testing data points
test.x = runif(1000, 0, 2*pi)
test.y = 2*sin(test.x) + rnorm(length(test.x))
test.y = test.y[order(test.x)]
test.x = test.x[order(test.x)]

# construct my functions
my_est <- function(X, x, y, lambda){
  u <- abs(X-x)/lambda
  K <- exp(-u^2/2)
  my_test.pred = sum(K*y)/sum(K)
  return(my_test.pred)
}
```

Then I used my code to fit the Nadaraya-Watson kernel regression estimator for my toy data example. Set the bandwidth as 0.5, and the fitting line is plotted as below:

```{r, echo = F, fig.align = 'center', fig.height=3.0, fig.width=5.0}
my_test.pred = sapply(test.x, function(X) my_est(X, x, y, 0.5))

# ggplot
test = data.frame(cbind(test.x, test.y))
my_test.fit = data.frame(cbind(test.x, my_test.pred))
# scatter plot
gplot = ggplot(test, aes(test$test.x, test$test.y))+ geom_point()+labs(x="test.x",y="test.y",title="My fitting line in toy example")+theme(plot.title = element_text(hjust = 0.5))
# my fit
gplot = gplot + geom_line(data=my_test.fit,aes(x=my_test.fit$test.x,y=my_test.fit$my_test.pred),color = "darkorange", lwd = 1.5)
gplot
```

To validate my code, I used the ksmooth function to fit the Nadaraya-Watson kernel regression estimator for this toy data example. While using the ksmooth function, we noticed that in the source code of this function, the bandwidth is multiplied by 0.3706506. Therefore, to accurately validate my code, I divided the bandwidth by 0.3706506. So I set the parameter as 0.5/0.3706506 in ksmooth function.

The fitting line derived by ksmooth function is plotted as below:

```{r, echo = F, fig.align = 'center', fig.height=3.0, fig.width=5.0}
# ksmooth
test.pred = ksmooth(x, y, kernel = "normal", bandwidth = 0.5/0.3706506, x.points = test.x)

# ggplot
test.fit = data.frame(cbind(test.pred$x, test.pred$y))
# scatter plot
gplot = ggplot(test, aes(test$test.x, test$test.y))+ geom_point()+labs(x="test.x",y="test.y",title="ksmooth fitting line in toy example")+theme(plot.title = element_text(hjust = 0.5))
# ksmooth fit
gplot = gplot + geom_line(data=test.fit,aes(x=test.fit$X1,y=test.fit$X2),color = "deepskyblue", lwd = 1.5)
gplot
```

Based on the two plots, we can see the fitting lines derived by my code and the ksmooth function are quite similar. To observe them more visually, I plot the two fitting lines in the same plot:

```{r, echo = F, fig.align = 'center', fig.height=3.0, fig.width=6.0}
# scatter plot
gplot = ggplot(test, aes(test$test.x, test$test.y))+ geom_point()+labs(x="test.x",y="test.y",title="ksmooth vs my code")+theme(plot.title = element_text(hjust = 0.5))
# ksmooth fit
gplot = gplot + geom_line(data=test.fit,aes(x=test.fit$X1,y=test.fit$X2,color = "ksmooth fit"), lwd = 1.5)
# my fit
gplot = gplot + geom_line(data=my_test.fit,aes(x=my_test.fit$test.x,y=my_test.fit$my_test.pred,color = "my fit"), lwd = 1.5)
gplot = gplot + scale_colour_manual(values = c("deepskyblue", "darkorange"))
gplot
```

In the above plot, the darkorange line is the fitting line derived by my code. It covers the deepskyblue line, which is the fitting line derived by the ksmooth function. Therefore, we can say the two lines are the same, which validates my code is correct.

## b)

Since there are many missing value and text value in the original dataset, I first did data cleaning and got a new dataset with 6947 observations of each variable to analyze. Then I shuffled the new dataset, and used my code to do 5-fold cross validation with different bandwidth.

```{r, echo = F, include = F}
video <- read_csv("C:/Users/guoshuhui/Desktop/542/homework/hw4/Video_Games_Sales_as_at_22_Dec_2016.csv")
video <- na.omit(video)
y <- log(1+video$Global_Sales)
cs <- video$Critic_Score
cc <- video$Critic_Count
us <- as.numeric(video$User_Score)
```

```{r, echo = F}
set.seed(1)
n = dim(video)[1]
sample <- sample(sequence(n))
video_new <- video[sample,]

nrFolds <- 5
# generate array containing fold-number for each sample (row)
folds <- rep_len(1:nrFolds, nrow(video_new))

my_cv <- function(x, C){
  my_error <- c()
  for (k in 1:nrFolds){
    fold <- which(folds == k)
    train_x <- x[-fold]
    test_x <- x[fold]
    train_y <- y[-fold]
    test_y <- y[fold]
    test.pred = sapply(test_x, function(X) my_est(X, train_x, train_y, C))
    my_error = c(my_error, mean((test_y-test.pred)^2))
  }
  return(mean(my_error))
}
```

To find the best bandwidth of each model, I first tried the bandwidth in a larger interval $[0.5,20.5]$ by 1 and report the MSE to compare. The trends of MSE with changes of bandwidth are plotted as below:

```{r, echo = F, fig.align = 'center', fig.height=2.7, fig.width=4.5}
# bandwidth
C <- seq(0.5, 20.5, 1)

error_cs <- c()
for (i in C){
  error_cs <- c(error_cs, my_cv(cs, i))
}

error_cc <- c()
for (i in C){
  error_cc <- c(error_cc, my_cv(cc, i))
}

error_us <- c()
for (i in C){
  error_us <- c(error_us, my_cv(us, i))
}

# plot
large_cs = data.frame(cbind(C, error_cs))
gplot = ggplot(large_cs, aes(large_cs$C, large_cs$error_cs))+labs(x="bandwidth",y="error",title="Critic Score")+theme(plot.title = element_text(hjust = 0.5))
gplot = gplot + geom_line(data=large_cs,aes(x=large_cs$C,y=large_cs$error_cs),color = "deepskyblue", lwd = 1.5)
gplot

large_cc = data.frame(cbind(C, error_cc))
gplot = ggplot(large_cc, aes(large_cc$C, large_cc$error_cc))+labs(x="bandwidth",y="error",title="Critic Count")+theme(plot.title = element_text(hjust = 0.5))
gplot = gplot + geom_line(data=large_cc,aes(x=large_cc$C,y=large_cc$error_cc),color = "deepskyblue", lwd = 1.5)
gplot

large_us = data.frame(cbind(C, error_us))
gplot = ggplot(large_us, aes(large_us$C, large_us$error_us))+labs(x="bandwidth",y="error",title="User Score")+theme(plot.title = element_text(hjust = 0.5))
gplot = gplot + geom_line(data=large_us,aes(x=large_us$C,y=large_us$error_us),color = "deepskyblue", lwd = 1.5)
gplot
```

In each of the above plot, we can find that there is a minimum MSE in the larger interval. And the bandwidth with the minimum MSE can be roughly spotted. For Critic_Score, it is `r C[which.min(error_cs)]`. For Critic_Count, it is `r C[which.min(error_cc)]`. For User_Score, it is `r C[which.min(error_us)]`.

To get the best bandwidth exactly, I tried a smaller interval for each variable. For Critic_Score, the new smaller interval is $[0.5,2.5]$ by 0.1. For Critic_Count, the new smaller interval is $[3.5,5.5]$ by 0.1. For User_Score, the smaller interval is $[0.1,2.0]$ by 0.1. The trends of MSE with changes of bandwidth are plotted as below:

```{r, echo = F, fig.align = 'center', fig.height=2.7, fig.width=4.5}
C_cs <- seq(0.5, 2.5, 0.1)
error2_cs <- c()
for (i in C_cs){
  error2_cs <- c(error2_cs, my_cv(cs, i))
}

C_cc <- seq(3.5, 5.5, 0.1)
error2_cc <- c()
for (i in C_cc){
  error2_cc <- c(error2_cc, my_cv(cc, i))
}

C_us <- seq(0.1, 2.0, 0.1)
error2_us <- c()
for (i in C_us){
  error2_us <- c(error2_us, my_cv(us, i))
}

# plot
small_cs = data.frame(cbind(C_cs, error2_cs))
gplot = ggplot(small_cs, aes(small_cs$C_cs, small_cs$error2_cs))+labs(x="bandwidth",y="error",title="Critic Score")+theme(plot.title = element_text(hjust = 0.5))
gplot = gplot + geom_line(data=small_cs,aes(x=small_cs$C_cs,y=small_cs$error2_cs),color = "deepskyblue", lwd = 1.5)
gplot

small_cc = data.frame(cbind(C_cc, error2_cc))
gplot = ggplot(small_cc, aes(small_cc$C_cc, small_cc$error2_cc))+labs(x="bandwidth",y="error",title="Critic Count")+theme(plot.title = element_text(hjust = 0.5))
gplot = gplot + geom_line(data=small_cc,aes(x=small_cc$C_cc,y=small_cc$error2_cc),color = "deepskyblue", lwd = 1.5)
gplot

small_us = data.frame(cbind(C_us, error2_us))
gplot = ggplot(small_us, aes(small_us$C_us, small_us$error2_us))+labs(x="bandwidth",y="error",title="User Score")+theme(plot.title = element_text(hjust = 0.5))
gplot = gplot + geom_line(data=small_us,aes(x=small_us$C_us,y=small_us$error2_us),color = "deepskyblue", lwd = 1.5)
gplot
```

In each of the above plot, we can find that there is a minimum MSE in the smaller interval. The bandwidth with the minimum MSE can be exactly spotted. For Critic_Score, the best bandwidth is `r C_cs[which.min(error2_cs)]`, the minimum MSE is `r round(min(error2_cs),4)`. For Critic_Count, the best bandwidth is `r C_cc[which.min(error2_cc)]`, the minimum MSE is `r round(min(error2_cc),4)`. For User_Score, the best bandwidth is `r C_us[which.min(error2_us)]`, the minimum MSE is `r round(min(error2_us),4)`.

Since the minumum MSE of Critic_Score is the smallest, Critic_Score gives the best model.


# Question 2

```{r, echo = F, include = F}
rm(list=ls())
library(MASS)
library(randomForest)
library(knitr)
library(ggplot2)
```

##a)

Generate $X$ from independent standard normal distribution with $n=200$ and $p=20$. The true model is

\begin{align*}
f(X)=1+0.5\sum_{j=1}^{4}X_{j}+\epsilon, \quad \epsilon \sim N(0,1)
\end{align*}

```{r, echo = F}
set.seed(1)
P = 20
N = 200
V <- diag(P)
X = as.matrix(mvrnorm(N, mu=rep(0,P), Sigma=V))

f = 1+0.5*(X[,1]+X[,2]+X[,3]+X[,4])
```

Define the degrees of freedom of a fit as $\sum_{i=1}^{n}Cov(\hat{y_i},y_i)/\sigma^2$. And choose $mtry=1,5,15$, $nodesize=5,15,25$ to estimate the degrees of freedom. To get better estimations, I performed this experiment for 20 times and calculated a sample covariance. Then I repeated the previous process 20 times to calculate the estimation and got the results below:

```{r, echo = F}
myfun <- function(times, mtry, nodesize){
  y <- matrix(0, times, N)
  yhat <- matrix(0, times, N)  #matrix of the estimations
  for(i in 1:times){
    Y = f+rnorm(N)
    y[i,] = Y
    
    rf.fit = randomForest(X, Y, mtry = mtry, nodesize = nodesize)
    yhat[i,] = predict(rf.fit, X)
  }
  
  #Calculate the covariance
  Ybar = f
  cov <- c()
  for (i in 1:N){
    cov[i] = sum((yhat[,i]-mean(yhat[,i]))%*%(y[,i]-Ybar[i]))/(times-1)
  }
  covariance = sum(cov)
  return(covariance)
}

set.seed(1)
mtry <- c(1, 5, 15)
nodesize <- c(5, 15, 25)

df <- matrix(0, length(mtry), length(nodesize))
for(i in 1:length(mtry)){
  for(j in 1:length(nodesize)){
    df[i,j] <- myfun(20, mtry[i], nodesize[j])
  }
}

df = round(df, 4)
rownames(df) <- c("mtry=1", "mtry=5", "mtry=15")
colnames(df) <- c("nodesize=5", "nodesize=15", "nodesize=25")
```

```{r, echo = F, results = 'asis'}
kable(df, align = 'c', caption = "degree of freedom")
```

Based on the results, we can find that the degree of freedom is correlated with mtry and nodesize. The degree of freedom increases as mtry increases and nodesize decreases. Mtry means the number of variables selected at each split. When mtry increases, there are more variables at each split and thus the degree of freedom increases. On the other hand, nodesize means the terminal node size where splitting stops. So when nodesize decreases, the degree of freedom increases. The above results make sense.

## b)

In this question, I fixed mtry and nodesize as the default value and chose ntree from 20 to 200 by 20. After repeating the estimation process for 20 times, I calculated the variances of this estimator using each value of ntree. The results are plotted as below:

```{r, echo = F, fig.align = 'center', fig.height=2.7, fig.width=4.5}
set.seed(1)
P = 20
N = 200
V <- diag(P)
X = as.matrix(mvrnorm(N, mu=rep(0,P), Sigma=V))

f = 1+0.5*(X[,1]+X[,2]+X[,3]+X[,4])
Y = f+rnorm(N)

myfun2 <- function(times, ntree){
  yhat <- matrix(0, times, N)  #matrix of the estimations
  for(i in 1:times){
    rf.fit = randomForest(X, Y, ntree = ntree)
    yhat[i,] = predict(rf.fit, X)
  }
  E_yhat = apply(yhat, 2, mean)
  dif = t(yhat)-E_yhat
  E = apply(dif^2, 1, mean)
  varr = mean(E)
  return(varr)
}

set.seed(1)
ntree <- seq(20, 200, 20)
variance = sapply(ntree, function(ntree) myfun2(20, ntree))

data <- data.frame(ntree, variance)
gplot = ggplot(data, aes(data$ntree, data$variance))+ geom_point()+labs(x="ntree",y="variance",title="variances of the estimator using each ntree value")+theme(plot.title = element_text(hjust = 0.5))
gplot
```

Based on the results, we can find that the variance is correlated with ntree. The variance decreases as ntree increases. Ntree means the number of trees builded to make estimation. When ntree increases, the estimation is made for more times, which increases the stability of the estimator and thus decreases the variance of the estimator. The above results make sense.


# Question 3

```{r, echo = F, include = F}
rm(list=ls())
library(ggplot2)
```

##a)

I wrote the function "stump" to fit the stump model with subject weights:

```{r}
stump <- function(x, y, w){
  n <- length(x)
  xnew <- sort(x)
  ynew <- y[order(x)]
  wnew <- w[order(x)]
  
  score <- c()
  fleft <- c()
  fright <- c()
  for (i in 1:n){
    # left
    xleft <- xnew[1:i]
    yleft <- ynew[1:i]
    wleft <- wnew[1:i]
    pleft = sum(wleft[yleft == 1])/sum(wleft)
    Gleft = pleft*(1-pleft)
    
    # right
    xright <- xnew[(i+1):n]
    yright <- ynew[(i+1):n]
    wright <- wnew[(i+1):n]
    pright = sum(wright[yright ==1])/sum(wright)
    Gright = pright*(1-pright)
    
    # score
    score[i] = -(sum(wleft)/sum(w))*Gleft-(sum(wright)/sum(w))*Gright
    
    # predictions
    fleft[i] <- (pleft >= 0.5)-(pleft < 0.5)
    fright[i] <- (pright >= 0.5)-(pright < 0.5)
  }
  # find the output
  c <- xnew[which.max(score)]
  fl <- fleft[which.max(score)]
  fr <- fright[which.max(score)]
  
  return(c(c, fl, fr))  
}
```

## b)

### Part I

Following the lecture slides page 6 in ¡°Boosting.pdf¡±, I first designed a model to train data and gave the outputs including $\alpha$, node predictions $f_{L}$, $f_{R}$ and the cutting point $c$. Then, I applied the outputs to fit the final model and give the output as:

\begin{align*}
F_{T}(x)=\sum_{t=1}^{T}\alpha_{t}f_{t}(x)
\end{align*}

Since I would first use this output instead of $sign(F_{T}(x))$ to test my code.

Then I generated $x$ and $y$ by the given code. I chose the iteration as $1, 10, 30, 100$ and got the final models. The results are plotted below.

In these plots, the grey line is the fitting line of $(sin(4\pi x)+1)/2$, where $x$ is from the data I genereated above. The red line is the estimated probability given as:

\begin{align*}
P(y=1|x)=\frac{e^{2F(x)}}{1+e^{2F(x)}}
\end{align*}

```{r, echo = F, fig.align = 'center'}
# training model
adaboost1 <- function(x.train, y.train, step){
  n <- length(x.train)
  w <- rep(1, n)/n
  
  c <- c()
  fl <- c()
  fr <- c()
  alpha <- c()
  for (t in 1:step){
    output <- stump(x.train, y.train, w)
    c[t] <- output[1]
    fl[t] <- output[2]
    fr[t] <- output[3]
    f <- (x.train <= c[t])*fl[t]+(x.train > c[t])*fr[t]
    epsilon = sum(w[y.train != f])
    alpha[t] = log((1-epsilon)/epsilon)/2
    z = sum(w*exp(-alpha[t]*y.train*f))
    w <- w*exp(-alpha[t]*y.train*f)/z
  }
  
  return(list(alpha = alpha, left = fl, right = fr, split = c))
}

adaboost2 <- function(x, ada_model, step){
  alpha = ada_model$alpha
  fl = ada_model$left
  fr = ada_model$right
  c = ada_model$split
  
  # final model
  fit <- rep(0,length(x))
  for (t in 1:step){
    temp = alpha[t]*((x <= c[t])*fl[t]+(x > c[t])*fr[t])
    fit = fit+temp
  }
  
  return(fit)
}

# generate data
set.seed(1)
n = 300
x = runif(n)
y = (rbinom(n, 1, (sin(4*pi*x)+1)/2)-0.5)*2

step1 = 100
ada_model <- adaboost1(x, y, step1)

fit1 = adaboost2(x, ada_model, 1)
fit2 = adaboost2(x, ada_model, 10)
fit3 = adaboost2(x, ada_model, 30)
fit4 = adaboost2(x, ada_model, 100)

p <- (sin(4*pi*x)+1)/2

# plot
data1 <- data.frame(x = x, my_p = 1/(1+exp(-2*fit1)), p = p)
gplot1 <- ggplot(data1)+geom_line(aes(x = sort(x), y = p[order(x)]), size = 2, color = "grey")
gplot1 <- gplot1+geom_line(aes(x = sort(x), y = my_p[order(x)]), size = 1, color = "red")
gplot1 <- gplot1+labs(title = "iteration=1", y = "")+theme(plot.title = element_text(hjust = 0.5))

data2 <- data.frame(x = x, my_p = 1/(1+exp(-2*fit2)), p = p)
gplot2 <- ggplot(data2)+geom_line(aes(x = sort(x), y = p[order(x)]), size = 2, color = "grey")
gplot2 <- gplot2+geom_line(aes(x = sort(x), y = my_p[order(x)]), size = 1, color = "red")
gplot2 <- gplot2+labs(title = "iteration=10", y = "")+theme(plot.title = element_text(hjust = 0.5))

data3 <- data.frame(x = x, my_p = 1/(1+exp(-2*fit3)), p = p)
gplot3 <- ggplot(data3)+geom_line(aes(x = sort(x), y = p[order(x)]), size = 2, color = "grey")
gplot3 <- gplot3+geom_line(aes(x = sort(x), y = my_p[order(x)]), size = 1, color = "red")
gplot3 <- gplot3+labs(title = "iteration=30", y = "")+theme(plot.title = element_text(hjust = 0.5))

data4 <- data.frame(x = x, my_p = 1/(1+exp(-2*fit4)), p = p)
gplot4 <- ggplot(data4)+geom_line(aes(x = sort(x), y = p[order(x)]), size = 2, color = "grey")
gplot4 <- gplot4+geom_line(aes(x = sort(x), y = my_p[order(x)]), size = 1, color = "red")
gplot4 <- gplot4+labs(title = "iteration=100", y = "")+theme(plot.title = element_text(hjust = 0.5))

cowplot::plot_grid(gplot1, gplot2, gplot3, gplot4, align = "v")
```

Based on the above plots, as the iteration increases, the red line can better fit the grey line, which validates that my code is correct.

### Part II

In this part, I set the output as the classification rule:

\begin{align*}
sign(F_{T}(x))
\end{align*}

Then I generated another dataset of $x$ and $y$ as testing data. Set the iteration as 2000, and calculate the misclassification error of training and testing data. The results are plotted as below:

```{r, echo = F, fig.align = 'center', fig.height=3.5, fig.width=6}
# generate training data
set.seed(1)
n = 300
x.train = runif(n)
y.train = (rbinom(n, 1, (sin(4*pi*x.train)+1)/2)-0.5)*2

# generate testing data
x.test = runif(n)
y.test = (rbinom(n, 1, (sin(4*pi*x.test)+1)/2)-0.5)*2

# calculate fitting results of training and testing data
step_new <- 2000
ada.model_new = adaboost1(x.train, y.train, step_new)

fit.train <- matrix(0, step_new, n)
fit.test <- matrix(0, step_new, n)
for (i in 1:step_new){
  fit.train[i,] = adaboost2(x.train, ada.model_new, i)
  fit.test[i,] = adaboost2(x.test, ada.model_new, i)
}

fit.train = sign(fit.train)
fit.test = sign(fit.test)

# calculate the misclassification error
error.train <- c()
error.test <- c()
for (i in 1:step_new){
  error.train[i] = sum(fit.train[i,] != y.train)/300
  error.test[i] = sum(fit.test[i,] != y.test)/300
}

error <- data.frame(step = seq(1, step_new, 1), error.train = error.train, error.test = error.test)
gplot <- ggplot(error)+geom_line(aes(x = step, y = error.train, color = "training"), size = 1)
gplot <- gplot+geom_line(aes(x = step, y = error.test, color = "testing"), size = 1)
gplot <- gplot+labs(x = "iteration",y = "misclassification error")
gplot <- gplot+labs(title = "Misclassification error of training and testing data")+theme(plot.title = element_text(hjust = 0.5))
gplot <- gplot + scale_colour_manual(values = c("deepskyblue", "darkorange"))
gplot
```

Based on the above plot, during the 2000 iterations, the misclassification error of training data keeps decreasing. However, the misclassification error of testing data first decreases then begins to increase after about 100 iterations, which shows there is overfitting after certain value of iteration.

In conclusion, adaboost model can significantly decrease the misclassification error after a long iteration. But for testing error, the overfitting problem could appear after a certain value of iteration. Therefore, the model would be much more better if the iteration value had been tuned.


---
title: "STAT 542, homework 5"
author: "Shuhui Guo"
fontsize: 12pt
output: pdf_document
---

# Question 1

\begin{align*}
(A-bb^{T})(A^{-1}+\frac{A^{-1}bb^{T}A^{-1}}{1-b^{T}A^{-1}b})&=AA^{-1}-bb^{T}A^{-1}+\frac{AA^{-1}bb^{T}A^{-1}-bb^{T}A^{-1}bb^{T}A^{-1}}{1-b^{T}A^{-1}b}\\
&=I-bb^{T}A^{-1}+\frac{bb^{T}A^{-1}-bb^{T}A^{-1}bb^{T}A^{-1}}{1-b^{T}A^{-1}b}\\
&=I-bb^{T}A^{-1}+\frac{(b-bb^{T}A^{-1}b)b^{T}A^{-1}}{1-b^{T}A^{-1}b}\\
&=I-bb^{T}A^{-1}+\frac{b(1-b^{T}A^{-1}b)b^{T}A^{-1}}{1-b^{T}A^{-1}b}\\
&=I-bb^{T}A^{-1}+bb^{T}A^{-1}\\
&=I
\end{align*}

Therefore, $(A-bb^{T})^{-1}=A^{-1}+\frac{A^{-1}bb^{T}A^{-1}}{1-b^{T}A^{-1}b}$

# Question 2

```{r, echo = F, include = F}
library(dr)
```

First I wrote my own code to fit the sliced inverse regression. To validate my code, I then generated X with 1000 observations and 10 variables from standard normal distribution. That is, the number of rows $n=1000$ and the number of columns $p=10$. Y is generated by:

\begin{align*}
y=0.125(x\beta)^{3}+0.5\epsilon, \quad \epsilon \sim N(0,1)
\end{align*}

where $\beta^{T}=(1,1,0,0,0,0,0,0,0,0)$.

Let scilces=10, the first 10 eigenvectors derived by my code is:

```{r, echo = FALSE}
# This function is to compute sigma^{1/2}
matrix_sqrt <- function(X, symmetric = FALSE) {
  # Perform the spectral decomposition.
  # Covariance matrices are symmetric
  X_eig <- eigen(X, symmetric = symmetric)
  
  # extract the Q eigen-vector matrix and the eigen-values
  Q <- X_eig$vectors
  values <- X_eig$values
  Q_inv <- solve(Q) # Q^{-1}
  
  Q %*% diag(sqrt(values)) %*% Q_inv
}

my_SIR <- function(X, Y, H){
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  E <- t(matrix(rep(colMeans(X), n), p, n))
  sigma <- t(X-E) %*% (X-E)/n
  # compute sigma^{-1/2}
  sigma_inv <- solve(matrix_sqrt(sigma, symmetric = TRUE))
  
  Z <- (X-E) %*% sigma_inv
  
  # sort the dataset (Z, Y) by the observed Y values
  Z = Z[order(Y), ]
  Y = Y[order(Y)]
  
  nrFolds <- H
  # generate array containing fold-number for each sample (row)
  folds <- split(seq_len(n),rep(1:H, each = floor(n/H), length.out=n))
  
  zh <- matrix(NA,H, p)
  nh <- c()
  for (i in 1:H){
    zh[i, ] <- colMeans(Z[folds[[i]],])
    nh[i] <- length(folds[[i]])
  }
  
  M <- t(zh) %*% apply(zh, 2, "*", nh)/sum(nh) #z_bar equals to 0, so it does not need to be here
  
  M_eig <- eigen(M, symmetric = TRUE)
  M_eigvec <- M_eig$vectors
  trans <- sigma_inv %*% M_eigvec
  
  # scale the trans, because the function 'dr' inculdes the scale process
  result <- scale(trans, center = FALSE, scale = sqrt(colSums(trans^2)))[1:p, 1:p]
  return(result)
}

set.seed(1)
n = 1000; p = 10
x = matrix(rnorm(n*p), n, p)
b = matrix(c(1, 1, rep(0, p-2)))
y = 0.125*(x %*% b)^3 + 0.5*rnorm(n)

my_SIR(x, y, 10)
```

The first 10 eigenvectors derived by the ¡°dr¡± package is:

```{r, echo = FALSE}
fit = dr(y~., data = data.frame(x, y), method = "sir", nslices=10)
fit$evectors
```

Based on the above results, the directions derived by my code and "dr" package are all the same. Although some results have opposite sign, the directions defined by eigenvectors are the same. So my code is validated to be correct.

## a)

In this question, the data X I generated using an underlying model that can be detected by SIR has 1000 observations and 10 variables. That is, the number of rows $n=1000$ and the number of columns $p=10$. X is generated by standard normal distribution. Y is generated by:

\begin{align*}
y=5sin(x\beta)+\epsilon, \quad \epsilon \sim N(0,1)
\end{align*}

where $\beta^{T}=(1,0,0,1,0,0,0,0,0,0)$.

My estimated direction is:

```{r, echo = FALSE}
set.seed(1)
n = 1000; p = 10
x = matrix(rnorm(n*p), n, p)
b = matrix(c(1, 0, 0, 1,rep(0, p-4)))
y = 5*sin(x %*% b) + rnorm(n)

est1 = my_SIR(x, y, 10)
```

\begin{align*}
\frac{\hat{\beta}_{2}}{\hat{\beta}_{1}}=`r round(est1[2,1]/est1[1,1], 2)` \quad \frac{\hat{\beta}_{3}}{\hat{\beta}_{1}}&=`r round(est1[3,1]/est1[1,1], 2)` \quad \frac{\hat{\beta}_{4}}{\hat{\beta}_{1}}=`r round(est1[4,1]/est1[1,1], 2)` \quad\\ \frac{\hat{\beta}_{5}}{\hat{\beta}_{1}}=`r round(est1[5,1]/est1[1,1], 2)` \quad \frac{\hat{\beta}_{6}}{\hat{\beta}_{1}}&=`r round(est1[6,1]/est1[1,1], 2)` \quad \frac{\hat{\beta}_{7}}{\hat{\beta}_{1}}=`r round(est1[7,1]/est1[1,1], 2)` \quad\\ \frac{\hat{\beta}_{8}}{\hat{\beta}_{1}}=`r round(est1[8,1]/est1[1,1], 2)` \quad \frac{\hat{\beta}_{9}}{\hat{\beta}_{1}}&=`r round(est1[9,1]/est1[1,1], 2)` \quad \frac{\hat{\beta}_{10}}{\hat{\beta}_{1}}=`r round(est1[10,1]/est1[1,1], 2)`
\end{align*}

The true direction is:

\begin{align*}
\frac{\beta_{2}}{\beta_{1}}=0 \quad \frac{\beta_{3}}{\beta_{1}}&=0 \quad \frac{\beta_{4}}{\beta_{1}}=1 \quad\\
\frac{\beta_{5}}{\beta_{1}}=0 \quad \frac{\beta_{6}}{\beta_{1}}&=0 \quad \frac{\beta_{7}}{\beta_{1}}=0 \quad\\
\frac{\beta_{8}}{\beta_{1}}=0 \quad \frac{\beta_{9}}{\beta_{1}}&=0 \quad \frac{\beta_{10}}{\beta_{1}}=0
\end{align*}

Based on the above results, my estimated direction are very close to true direction. So the SIR could detect the underlying model I generated before. It is because that the generating function $y=5sin(x\beta)$ is a linear function. Therefore, SIR can detect the underlying model by its first moment information.

## b)

In this question, the data X I generated using an underlying model that cannot be detected by SIR has 1000 observations and 10 variables. That is, the number of rows $n=1000$ and the number of columns $p=10$. X is generated by standard normal distribution. Y is generated by:

\begin{align*}
y=3(cos(x\beta))^{2}+\epsilon, \quad \epsilon \sim N(0,1)
\end{align*}

where $\beta^{T}=(1,1,1,0,0,0,0,0,0,0)$.

My estimated direction is:

```{r, echo = FALSE}
set.seed(1)
n = 1000; p = 10
x = matrix(rnorm(n*p), n, p)
b = matrix(c(1, 1, 1, rep(0, p-3)))
y = 3*(cos(x %*% b))^2 + rnorm(n)

est2 = my_SIR(x, y, 10)
```

\begin{align*}
\frac{\hat{\beta}_{2}}{\hat{\beta}_{1}}=`r round(est2[2,1]/est2[1,1], 2)` \quad \frac{\hat{\beta}_{3}}{\hat{\beta}_{1}}&=`r round(est2[3,1]/est2[1,1], 2)` \quad \frac{\hat{\beta}_{4}}{\hat{\beta}_{1}}=`r round(est2[4,1]/est2[1,1], 2)` \quad\\ \frac{\hat{\beta}_{5}}{\hat{\beta}_{1}}=`r round(est2[5,1]/est2[1,1], 2)` \quad \frac{\hat{\beta}_{6}}{\hat{\beta}_{1}}&=`r round(est2[6,1]/est2[1,1], 2)` \quad \frac{\hat{\beta}_{7}}{\hat{\beta}_{1}}=`r round(est2[7,1]/est2[1,1], 2)` \quad\\ \frac{\hat{\beta}_{8}}{\hat{\beta}_{1}}=`r round(est2[8,1]/est2[1,1], 2)` \quad \frac{\hat{\beta}_{9}}{\hat{\beta}_{1}}&=`r round(est2[9,1]/est2[1,1], 2)` \quad \frac{\hat{\beta}_{10}}{\hat{\beta}_{1}}=`r round(est2[10,1]/est2[1,1], 2)`
\end{align*}

The true direction is:

\begin{align*}
\frac{\beta_{2}}{\beta_{1}}=1 \quad \frac{\beta_{3}}{\beta_{1}}&=1 \quad \frac{\beta_{4}}{\beta_{1}}=1 \quad\\
\frac{\beta_{5}}{\beta_{1}}=0 \quad \frac{\beta_{6}}{\beta_{1}}&=0 \quad \frac{\beta_{7}}{\beta_{1}}=0 \quad\\
\frac{\beta_{8}}{\beta_{1}}=0 \quad \frac{\beta_{9}}{\beta_{1}}&=0 \quad \frac{\beta_{10}}{\beta_{1}}=0
\end{align*}

Based on the above results, my estimated direction are quitely different from true direction. So the SIR could not detect this underlying model. It is because that the generating function $y=3(cos(x\beta))^{2}$ is not a linear function. SIR can not detect the underlying model by its first moment information, which is the way that SIR applies. Therefore, SIR cannot detect this underlying model.

# Question 3

In this part, there are two questions to be settled. One is to solve a regression problem to predict the variable \textbf{revenue}. The other one is to slove a classification problem to predict whether the \textbf{vote\_average} is greater than 7.

To deal with these problems, first I conduct variable selections, including the following steps:

i) Remove the variables \textbf{vote\_count} and \textbf{popularity}. The information in these variables is after the release date so they should not be used to do prediction.

ii) Remove the variables \textbf{homepage}, \textbf{original\_language}, \textbf{original\_title}, \textbf{overview}, \textbf{release\_date}, \textbf{spoken\_language}, \textbf{status}, \textbf{tagline}, \textbf{title} because they cannot provide much information to solve the two questions.

So the variables I will use for prediction are: \textbf{budget}, \textbf{genres}, \textbf{keywords}, \textbf{production\_companies}, \textbf{production\_countries}, \textbf{runtime}.

Then I preprocess the data in the following steps:

i) Expand the categorical variables \textbf{genres}, \textbf{keywords}, \textbf{production\_companies}, \textbf{production\_countries} into dummy variables which only contain 0 and 1 levels in each variable. For example, for the variable \textbf{genres}, there are 20 types such as "Action", "Adventure", "Fantasy". Therefore \textbf{genres} could be transformed into 20 dummy variables. If one type is included in genres of a movie, the level of this type would be 1, otherwise the level would be 0. 

ii) Remove the dummy variables containing all level 0 or only one level 1. Since these variables cannot provide much infromation to prediction.

iii) Select the variables whose correlation with \textbf{revenue}, \textbf{vote\_average} is not small. For the first question to predict \textbf{revenue}, the threshold of correlation is 0.1. The variables whose correlation with \textbf{revenue} not less than 0.1 are selected. For the second question to predict \textbf{vote\_average}, the threshold is 0.05. The variables whose correlation with \textbf{vote\_average} not less than 0.05 are selected.

After above steps, I will fit models to each question.

```{r, echo = F, cache=T}
rm(list=ls())
```

## Part I. Predict Revenue

```{r, echo = F, include = F}
library(glmnet)
library(ggplot2)
library(randomForest)
```

```{r, echo = F, cache=T}
load("use_data.RData")
data_temp <- data[, -(1:20)]
data_use <- cbind(data_total[c(1,4,13,14)], data_temp)

data_use <- data_use[data_use$revenue != 0, ]
Y <- data_use$revenue
X <- data_use[, -c(2, 3)]

# there are two numeric variables 'budget' and 'runtime' in X, set them as numeric
X[,1] <- as.numeric(X[,1])
X[,2] <- as.numeric(X[,2])

# odd id as training, even id as testing
Y_training <- Y[data_use$id %% 2 == 1]
Y_testing <- Y[data_use$id %% 2 == 0]
X_training <- X[data_use$id %% 2 == 1, ]
X_testing <- X[data_use$id %% 2 == 0, ]

# select the variables whose sum values are more than one
# since they have meaning under this circumstance
X_testing <- X_testing[,colSums(X_training) > 1]
X_training <- X_training[,colSums(X_training) > 1]

# calculate the correlation between X and Y training data
# select the training x whose correlation with y is not less than 0.1
# select the testing x according to the previous step
r <- abs(cor(Y_training, X_training))
X_training <- X_training[, r >= 0.1]
X_testing <- X_testing[, r >= 0.1]
```

In this question, first I will remove the observations whose revenue equals 0 since these are missing values and should not be included in the prediction. Then I will try two models, lasso and random forest to do prediction. Finally I will use the mean absolute error to compare their prediction effect and select the model with the smallest error to make conclusions.

### Lasso

```{r, echo = F, cache=T}
## lasso
X_training <- as.matrix(X_training)
X_testing <- as.matrix(X_testing)
m1.cv <- cv.glmnet(X_training, Y_training, alpha = 1, nfolds = 10)

# save lambda and fit lasso with this lambda
lambda <- m1.cv$lambda.min
m1 <- glmnet(X_training, Y_training, alpha = 1, lambda = lambda)
Y_pre1 <- predict(m1, X_testing)
error_m1 <- mean(abs(Y_pre1-Y_testing))
```

For the lasso model, I first use training data to do 10 folds cross validation to find the best $\lambda$, which is $log(\lambda)= `r round(log(lambda))`$. Then I use the model with this $\lambda$ to fit testing data and get prediction values. The comparison of testing data and prediction values is plotted as below:

```{r, echo = F, fig.align = 'center', cache=T}
# plot
data_m1 <- data.frame(x = seq(1,length(Y_pre1),1), s0 = Y_pre1, s1 = Y_testing)
gplot <- ggplot(data_m1) + geom_line(aes(x = x, y = s1, color = "true value"))
gplot <- gplot + geom_line(aes(x = x, y = s0, color = "predicted value"))
gplot <- gplot + labs(x = "sequence",y = "revenue")
gplot <- gplot + labs(title = "lasso") + theme(plot.title = element_text(hjust = 0.5))
gplot <- gplot + scale_colour_manual(values = c("deepskyblue", "darkorange"))
gplot
```

In this plot, the trend of testing data is well fitted by prediction values. The prediction error is $`r error_m1`$.

### Random Forest

For random forest model, I set the parameter $ntree=1000$, $mtry=p/3=139/3$, $nodesize=5$. Then I use this model to fit testing data and get prediction values. The comparison of testing data and prediction values is plotted as below:

```{r, echo = F, fig.align = 'center', cache=T}
m2 = randomForest(X_training, Y_training, ntree = 1000, mtry = 139/3, nodesize = 5) # mtry=p/3
Y_pre2 <- predict(m2, X_testing)
error_m2 <- mean(abs(Y_pre2-Y_testing))

# plot
data_m2 <- data.frame(x = seq(1,length(Y_pre2),1), s0 = Y_pre2, s1 = Y_testing)
gplot <- ggplot(data_m2) + geom_line(aes(x = x, y = s1, color = "true value"))
gplot <- gplot + geom_line(aes(x = x, y = s0, color = "predicted value"))
gplot <- gplot + labs(x = "sequence",y = "revenue")
gplot <- gplot + labs(title = "random forest") + theme(plot.title = element_text(hjust = 0.5))
gplot <- gplot + scale_colour_manual(values = c("deepskyblue", "darkorange"))
gplot
```

In this plot, the trend of testing data is well fitted by prediction values. The prediction error is $`r error_m2`$.

### Model Selection

Based on the above results, the prediction error of random forest is much smaller, which is $`r error_m2`$. So random forest is chosen to be the final model.

The importance of predictors given by random forest can be shown as below:

```{r, echo = F, fig.height=6,  fig.width=7.8, cache=T}
# plot the variable importance
varImpPlot(m2)
```

Based on this plot, there are 6 predictor that are relatively more important in this model. The most important predictor is \textbf{budget}. It matches the real condition because whether the movie production is excellent is mostly decided by the budget. And the excellence of movie production is closely related to movie's revenue. The second important predictor is \textbf{runtime}. The runtime of a movie is related to the complexity and richness of content so that it is important for revenue. The third important predictor is \textbf{Adventure} in \textbf{genres}. The fourth important predictor \textbf{Lightstorm.Entertainment} and fifth important predictor \textbf{Fuji.Television.Network} are names of production companies. And the sixth important predictor \textbf{mind.and.soul} is a type of key words of movies.

## Part II. Predict vote_average

```{r, echo = F, include = F}
rm(list=ls())
library(glmnet)
library(e1071)
library(randomForest)
```

```{r, echo = F, cache=T}
load("use_data.RData")
data2_temp <- data[, -(1:20)]
data2_use <- cbind(data_total[c(1,4,19,14)], data2_temp)

Y <- data2_use$vote_average
Y <- as.numeric(paste(Y))

# set vote_average>7 as 1, <7 as 0
Y <- as.numeric(Y > 7)
X <- data2_use[, -c(2, 3)]

# there are two numeric variables 'budget' and 'runtime' in X, set them as numeric
X[,1] <- as.numeric(X[,1])
X[,2] <- as.numeric(X[,2])

# odd id as training, even id as testing
Y_training <- Y[data2_use$id %% 2 == 1]
Y_testing <- Y[data2_use$id %% 2 == 0]
X_training <- X[data2_use$id %% 2 == 1, ]
X_testing <- X[data2_use$id %% 2 == 0, ]

# select the variables whose sum values are more than one
# since they have meaning under this circumstance
X_testing <- X_testing[,colSums(X_training) > 1]
X_training <- X_training[,colSums(X_training) > 1]

# calculate the correlation between X and Y training data
# select the training x whose correlation with y is not less than 0.1
# select the testing x according to the previous step
r <- abs(cor(Y_training, X_training))
X_training <- X_training[, r >= 0.05]
X_testing <- X_testing[, r >= 0.05]

# there are much more 0 than 1, so we should balance these unbalanced data
Y_training1 <- Y_training[Y_training == 1]
X_training1 <- X_training[Y_training == 1,]
Y_training0 <- Y_training[Y_training == 0]
X_training0 <- X_training[Y_training == 0,]

set.seed(1)
temp <- sample(seq(1,length(Y_training0),1), 1.8*length(Y_training1))
Y_training <- as.factor(c(Y_training1, Y_training0[temp]))
X_training <- rbind(X_training1, X_training0[temp,])
```

In this question, first I will classify expand \textbf{vote\_average} to a dummy variable that 1 is value of \textbf{vote\_average} greater than 7 and otherwise is 0. However, the rate of level 1 is around 0.174 while the rate of level 0 is around 0.83, which are unbalanced. So I will use all level 1 data and randomly select the same amount of level 0 data to be training data. Thus they are balanced to do prediction. Then I will try three models, lasso, SVM and random forest to do prediction. Finally I will use the misclassification error to compare their prediction effect and select the model with the smallest error to make conclusions.

### Lasso

```{r, echo = F, cache=T}
X_training <- as.matrix(X_training)
X_testing <- as.matrix(X_testing[-c(1308,2033), ]) #the rows of 1308 and 2033 will introduce NA, so remove them
Y_testing <- Y_testing[-c(1308,2033)]
m1.cv <- cv.glmnet(X_training, Y_training, family=c("binomial"), alpha = 1, nfolds = 10)

# save lambda and fit lasso with this lambda
lambda <- m1.cv$lambda.min
m1 <- glmnet(x=X_training, y=Y_training, family=c("binomial"), alpha = 1, lambda = lambda)
Y_pre1 <- predict(m1, X_testing, type = "class")
error_m1 <- mean(Y_pre1 != Y_testing)
```

For the lasso model, I first use training data to do 10 folds cross validation to find the best $\lambda$, which is $`r round(lambda)`$. Then I use the model with this $\lambda$ to fit testing data and get prediction values. The misclassification error is $`r error_m1`$.

### SVM

```{r, echo = F, cache=T}
cost <- seq(1,31,5)
error <- c()
for(i in cost){
  m2.fit <- svm(y ~ ., data = data.frame(X_training, y=Y_training),type='C-classification', kernel='linear', scale=FALSE, cost = i)
  Y_pre2 <- predict(m2.fit, X_testing)
  error <- c(error, mean(Y_pre2 != Y_testing))
}

m2 <- svm(y ~ ., data = data.frame(X_training, y=Y_training), type='C-classification', kernel='linear',scale=FALSE, cost = cost[which.min(error)])
Y_prem2 <- predict(m2, X_testing)
error_m2 <- mean(Y_prem2 != Y_testing)
```

For SVM, I first try the cost value from 1 to 31 by 5 and compare their misclassification error to find the best model. Then I use this model to fit testing data and get prediction values. The misclassification error is $`r error_m2`$. And the cost for selected model is $`r cost[which.min(error)]`$.

### Random Forest

```{r, echo = F, cache=T}
m3 = randomForest(X_training, Y_training, ntree = 1000, mtry = 351/3, nodesize = 5)
Y_pre3 <- predict(m3, X_testing)
error_m3 <- mean(Y_pre3 != Y_testing)
```

For random forest model, I set the parameter $ntree=1000$, $mtry=p/3=351/3$, $nodesize=5$. Then I use this model to fit testing data and get prediction values. The misclassification error is $`r error_m3`$.

### Model Selection

Based on the above results, the misclassification error of lasso is the smallest, which is $`r error_m1`$. So lasso is chosen to be the final model.

Since the larger the coefficient of a variable, the more influence this variable could exert on the prediction values. Hence I find the variables with the largest 20 coefficients, which are relatively important predictors:

```{r, echo = F, cache=T}
## select the important predictors
## since the first one is intercept and our main focus is on the variables
## so I removed the intercept and set the sequence as 2:21
as.vector(coef(m1)@Dimnames[[1]][order(coef(m1))[2:21]])
```

From the results, we could know that the top 20 important predictors mostly belong to \textbf{genres} and \textbf{keywords}. Also there is a predictor \textbf{United.States.of.America} belonging to \textbf{production\_countries}. However, there are no \textbf{budget} and \textbf{runtime} like part I. The reason could be that the influence of these two variables on average rating is not much. On the other hand, audiences focus more on movies' genres and contents while they are rating movies.

## Predict "Star Wars: The Last Jedi"

```{r, echo = F, cache=T}
rm(list=ls())
```

```{r, echo = F, include = F}
library(randomForest)
```

```{r, echo = F, cache=T}
load("use_data.RData")
data_temp <- data[, -(1:20)]
data_use <- cbind(data_total[c(1,4,13,14)], data_temp)

data_use <- data_use[data_use$revenue != 0, ]
Y <- data_use$revenue
X <- data_use[, -c(2, 3)]

# there are two numeric variables 'budget' and 'runtime' in X, set them as numeric
X[,1] <- as.numeric(X[,1])
X[,2] <- as.numeric(X[,2])

# odd id as training, even id as testing
Y_training <- Y[data_use$id %% 2 == 1]
Y_testing <- Y[data_use$id %% 2 == 0]
X_training <- X[data_use$id %% 2 == 1, ]
X_testing <- X[data_use$id %% 2 == 0, ]

# select the variables whose sum values are more than one
# since they have meaning under this circumstance
X_testing <- X_testing[,colSums(X_training) > 1]
X_training <- X_training[,colSums(X_training) > 1]

# calculate the correlation between X and Y training data
# select the training x whose correlation with y is not less than 0.1
# select the testing x according to the previous step
r <- abs(cor(Y_training, X_training))
X_training <- X_training[, r >= 0.1]
X_testing <- X_testing[, r >= 0.1]

## predict star war 8
m2 = randomForest(X_training, Y_training, ntree = 1000, mtry = 139/3, nodesize = 5) # mtry=p/3
starwar <- X_testing[1,]
starwar[1,1:139] <- 0
starwar$budget <- 245000000
starwar$runtime <- 152
starwar$Action <- 1
starwar$Fantasy <- 1
starwar$Adventure <- 1
starwar_pre <- as.numeric(predict(m2, starwar))
```

For the revenue part, I use random forest selected in Part I. This movie's information included in the important predictors selected by random forest are $\textbf{budget}=2.45\times 10^{8}$, $\textbf{runtime}=152$, and \textbf{Adventure}, \textbf{Action}, \textbf{Fantasy} type in \textbf{genres}. Hence, I use these five predictors to do the prediction. The predicted revenue is $`r starwar_pre`$.

For the rating part, since the information released is not included in the important predictors selected by lasso in Part II, it is not easy to do prediction using existed models. Nevertheless, the rating could still be predicted as greater than 7. Since the rating of previous "Star Wars" series are all greater than 7 and the budget of this "Star Wars" movie is so high that the movie production could be excellent. Therefore, the rating could be greater than 7.
